.. knowledge_record documentation master file, created by
   sphinx-quickstart on Tue July 4 21:15:34 2020.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

******************
Machine_Learning
******************

ensemble
=====================

änˈsämbəl

分为bagging(减小方差)、boosting(偏差) 和 stacking(改进预测)

bagging和boost
---------------------
| Bagging算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林。

| Boosting的意思是这样，他通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。其代表算法是AdaBoost, GBDT。

| Bagging和Boosting的区别（面试准备）
| https://www.cnblogs.com/earendil/p/8872001.html

stacking
--------------------

.. image:: ../../_static/machine_learning/stacking.png
	:align: center
	
关于stacking 的使用
https://blog.csdn.net/Li_yi_chao/article/details/89638009
	

GBDT、Adboost、xgboost
-------------------------
| GBDT与Adboost最主要的区别在于两者如何识别模型的问题。Adaboost用错分数据点来识别问题，通过调整错分数据点的权重来改进模型。GBDT通过负梯度来识别问题，通过计算负梯度来改进模型。

| 推荐GBDT树的深度：6；（横向比较：DecisionTree/RandomForest需要把树的深度调到15或更高）

| 提升树与梯度提升树：利用损失函数的负梯度在当前模型的值，代替提升树算法中的残差的作用


| GBDT 台大林轩田老师讲解视频：
| https://www.youtube.com/watch?v=pTNKUj_1Dw8&list=PL1AVtvtzG0LYN-dOGPYyRrzzyI5fk_D4H&index=31

| 这个文字资料写的不错：
| http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/

| AdaBoost算法缺点

| 对异常点敏感
| 最终模型无法用概率来解释


| Xgboost 为什么用二阶泰勒展开

| 二阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法里已经证实了。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。这是从二阶导本身的性质，也就是为什么要用泰勒二阶展开的角度来说的

| 收敛速度上有提升

| 知乎：最优化问题中，牛顿法为什么比梯度下降法求解需要的迭代次数更少？https://www.zhihu.com/question/19723347

| Xgboost https://juejin.im/post/5d2590e1e51d45106b15ffaa 这篇文章讲的不错


| XGBoost与GBDT有什么不同

| 除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。
| 1.	GBDT是机器学习算法，XGBoost是该算法的工程实现。
| 2.	在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和
| 3.	GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。
| 4.	传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。
| 5.	传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。
| 6.	传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。


决策树与这些算法框架进行结合所得到的新的算法：

| 1）Bagging + 决策树 = 随机森林
| 2）AdaBoost + 决策树 = 提升树
| 3）Gradient Boosting + 决策树 = GBDT

xgboost怎么给特征评分？

| 在训练的过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。
| 特征评分可以看成是被用来分离决策树的次数。


基础机器学习算法
========================

Logistics regression
----------------------------
李宏毅视频


.. image:: ../../_static/machine_learning/lr.png
	:align: center
	
| 为什么 logistic regression 的输入特征一般是离散的而不是连续的？
| （1）离散特征的增加和减少都很容易，易于模型的快速迭代。 
| （2）稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。 
| （3）对异常数据具有较强的鲁棒性。 
| （4）单个特征离散化为 N 个后，每个特征有单独的权重，相当于引入了非线性，增加了模型的表达能力，加大了拟合能力。 
| （5）可以特征交叉，M + N 个特征变为 M * N 个特征，进一步引入非线性，提升表达能力。 
| （6）特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。


SVM
-------------
| https://www.bilibili.com/video/BV1ut41197F6?p=14
| 林轩田的 
| 包括李航的统计学习

.. image:: ../../_static/machine_learning/SVM.png
	:align: center
	

.. image:: ../../_static/machine_learning/hinge_loss.png
	:align: center

	
聚类
-------------
DBSCAN

kmeans

GMM

两种聚类的思想，评价，具体实现

https://www.nowcoder.com/discuss/432266?type=post&order=create&pos=&page=0&channel=666&source_id=search_post

https://www.jianshu.com/p/78e9e1b8553a


贝叶斯
----------------
李航统计学习
	
https://www.zhihu.com/question/19725590/answer/241988854



决策树与随机森林等相关知识点
--------------------------------------------

随机森林面试题

1.1 优缺点

| 优点。
| (1)不必担心过度拟合；
| (2)适用于数据集中存在大量未知特征；
| (3)能够估计哪个特征在分类中更重要；
| (4)具有很好的抗噪声能力；
| (5)算法容易理解；
| (6)可以并行处理。

| 缺点。
| （1）对小量数据集和低维数据集的分类不一定可以得到很好的效果。
| （2）执行速度虽然比Boosting等快，但是比单个的决策树慢很多。
| （3）可能会出现一些差异度非常小的树，淹没了一些正确的决策。
| （4）由于树是随机生成的，结果不稳定（kpi值比较大）

| 1.2 生成步骤介绍
| 1、从原始训练数据集中，应用bootstrap方法有放回地随机抽取k个新的自助样本集，并由此构建k棵分类回归树，每次未被抽到的样本组成了Ｋ个袋外数据（out-of-bag,BBB）。
| 2、设有n 个特征，则在每一棵树的每个节点处随机抽取mtry 个特征，通过计算每个特征蕴含的信息量，特征中选择一个最具有分类能力的特征进行节点分裂。
| 3、每棵树最大限度地生长， 不做任何剪裁
| 4、将生成的多棵树组成随机森林， 用随机森林对新的数据进行分类， 分类结果按树分类器投票多少而定。

| 1.3 随机森林与SVM的比较
| （1）不需要调节过多的参数，因为随机森林只需要调节树的数量，而且树的数量一般是越多越好，而其他机器学习算法，比如SVM，有非常多超参数需要调整，如选择最合适的核函数，正则惩罚等。
| （2）分类较为简单、直接。随机森林和支持向量机都是非参数模型（复杂度随着训练模型样本的增加而增大）。相较于一般线性模型，就计算消耗来看，训练非参数模型因此更为耗时耗力。分类树越多，需要更耗时来构建随机森林模型。同样，我们训练出来的支持向量机有很多支持向量，最坏情况为，我们训练集有多少实例，就有多少支持向量。虽然，我们可以使用多类支持向量机，但传统多类分类问题的执行一般是one-vs-all（所谓one-vs-all 就是将binary分类的方法应用到多类分类中。比如我想分成K类，那么就将其中一类作为positive），因此我们还是需要为每个类训练一个支持向量机。相反，决策树与随机深林则可以毫无压力解决多类问题。
| （3）比较容易入手实践。随机森林在训练模型上要更为简单。你很容易可以得到一个又好且具鲁棒性的模型。随机森林模型的复杂度与训练样本和树成正比。支持向量机则需要我们在调参方面做些工作，除此之外，计算成本会随着类增加呈线性增长。
| （4）小数据上，SVM优异，而随机森林对数据需求较大。就经验来说，我更愿意认为支持向量机在存在较少极值的小数据集上具有优势。随机森林则需要更多数据但一般可以得到非常好的且具有鲁棒性的模型。

| 1.4 随机森林不会发生过拟合的原因
| 在建立每一棵决策树的过程中，有两点需要注意-采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。
| 对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。*然后进行列采样，从M 个feature中，选择m个(m << M)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一 个分类。*一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。

| 1.5 随机森林与梯度提升树（GBDT）区别
| 随机森林：决策树+bagging=随机森林
| 梯度提升树：决策树+Boosting=GBDT
| 两者区别在于bagging boosting之间的区别。
| 像神经网络这样为消耗时间的算法，bagging可通过并行节省大量的时间开销
| baging和boosting都可以有效地提高分类的准确性
| baging和boosting都可以有效地提高分类的准确性
| 一些模型中会造成模型的退化（过拟合）
| boosting思想的一种改进型adaboost方法在邮件过滤，文本分类中有很好的性能。

决策树

ID3算法用的是信息增益，C4.5算法用信息增益率；CART算法使用基尼系数

三个的具体细节？？？待补充


其他常见问题
======================

如何解决机器学习中样本不均衡问题？
------------------------------------------
| •	通过过抽样和欠抽样解决样本不均衡

| 抽样是解决样本分布不均衡相对简单且常用的方法，包括过抽样和欠抽样两种。

| 过抽样
| 过抽样（也叫上采样、over-sampling）方法通过增加分类中少数类样本的数量来实现样本均衡，最直接的方法是简单复制少数类样本形成多条记录，这种方法的缺点是如果样本特征少而可能导致过拟合的问题；经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本，例如SMOTE算法。

| 欠抽样
| 欠抽样（也叫下采样、under-sampling）方法通过减少分类中多数类样本的样本数量来实现样本均衡，最直接的方法是随机地去掉一些多数类样本来减小多数类的规模，缺点是会丢失多数类样本中的一些重要信息。

| 总体上，过抽样和欠抽样更适合大数据分布不均衡的情况，尤其是第一种（过抽样）方法应用更加广泛。

| •	通过正负样本的惩罚权重解决样本不均衡

| 通过正负样本的惩罚权重解决样本不均衡的问题的思想是在算法实现过程中，对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。
| 使用这种方法时需要对样本本身做额外处理，只需在算法模型的参数中进行相应设置即可。很多模型和算法中都有基于类别参数的调整设置，以scikit-learn中的SVM为例，通过在class_weight
| : {dict, 'balanced'}中针对不同类别针对不同的权重，来手动指定不同类别的权重。如果使用其默认的方法balanced，那么SVM会将权重设置为与不同类别样本数量呈反比的权重来做自动均衡处理，计算公式为：n_samples / (n_classes * np.bincount(y))。
| 如果算法本身支持，这种思路是更加简单且高效的方法。

| •	通过组合/集成方法解决样本不均衡
| 组合/集成方法指的是在每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果。
| 例如，在数据集中的正、负例的样本分别为100和10000条，比例为1:100。此时可以将负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据；然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。
| 这种解决问题的思路类似于随机森林。在随机森林中，虽然每个小决策树的分类能力很弱，但是通过大量的“小树”组合形成的“森林”具有良好的模型预测能力。
| 如果计算资源充足，并且对于模型的时效性要求不高的话，这种方法比较合适。

| •	通过特征选择解决样本不均衡
| 上述几种方法都是基于数据行的操作，通过多种途径来使得不同类别的样本数据行记录均衡。除此以外，还可以考虑使用或辅助于基于列的特征选择方法。
| 一般情况下，样本不均衡也会导致特征分布不均衡，但如果小类别样本量具有一定的规模，那么意味着其特征值的分布较为均匀，可通过选择具有显著型的特征配合参与解决样本不均衡问题，也能在一定程度上提高模型效果。
| 提示 上述几种方法的思路都是基于分类问题解决的。实际上，这种从大规模数据中寻找罕见数据的情况，也可以使用非监督式的学习方法，例如使用One-class SVM进行异常检测。分类是监督式方法，前期是基于带有标签（Label）的数据进行分类预测；而采用非监督式方法，则是使用除了标签以外的其他特征进行模型拟合，这样也能得到异常数据记录。所以，要解决异常检测类的问题，先是考虑整体思路，然后再考虑方法模型。


数据挖掘中常见的「异常检测」算法有哪些？
------------------------------------------------
| https://www.zhihu.com/question/280696035
| 1. 无监督异常检测

| 如果归类的话，无监督异常检测模型可以大致分为：

| •	统计与概率模型（statistical and probabilistic and models）：主要是对数据的分布做出假设，并找出假设下所定义的“异常”，因此往往会使用极值分析或者假设检验。比如对最简单的一维数据假设高斯分布，然后将距离均值特定范围以外的数据当做异常点。而推广到高维后，可以假设每个维度各自独立，并将各个维度上的异常度相加。如果考虑特征间的相关性，也可以用马氏距离（mahalanobis distance）来衡量数据的异常度[12]。不难看出，这类方法最大的好处就是速度一般比较快，但因为存在比较强的“假设”，效果不一定很好。

| •	线性模型（linear models）：假设数据在低维空间上有嵌入，那么无法、或者在低维空间投射后表现不好的数据可以认为是离群点。举个简单的例子，PCA可以用于做异常检测[10]，一种方法就是找到k个特征向量（eigenvector），并计算每个样本再经过这k个特征向量投射后的重建误差（reconstruction error），而正常点的重建误差应该小于异常点。同理，也可以计算每个样本到这k个选特征向量所构成的超空间的加权欧氏距离（特征值越小权重越大）。在相似的思路下，我们也可以直接对协方差矩阵进行分析，并把样本的马氏距离（在考虑特征间关系时样本到分布中心的距离）作为样本的异常度，而这种方法也可以被理解为一种软性（Soft PCA） [6]。同时，另一种经典算法One-class SVM[3]也一般被归类为线性模型。

| •	基于相似度衡量的模型（proximity based models）：异常点因为和正常点的分布不同，因此相似度较低，由此衍生了一系列算法通过相似度来识别异常点。比如最简单的K近邻就可以做异常检测，一个样本和它第k个近邻的距离就可以被当做是异常值，显然异常点的k近邻距离更大。同理，基于密度分析如LOF [1]、LOCI和LoOP主要是通过局部的数据密度来检测异常。显然，异常点所在空间的数据点少，密度低。相似的是，Isolation Forest[2]通过划分超平面来计算“孤立”一个样本所需的超平面数量（可以想象成在想吃蛋糕上的樱桃所需的最少刀数）。在密度低的空间里（异常点所在空间中），孤例一个样本所需要的划分次数更少。另一种相似的算法ABOD[7]是计算每个样本与所有其他样本对所形成的夹角的方差，异常点因为远离正常点，因此方差变化小。换句话说，大部分异常检测算法都可以被认为是一种估计相似度，无论是通过密度、距离、夹角或是划分超平面。通过聚类也可以被理解为一种相似度度量，比较常见不再赘述。

| •	集成异常检测与模型融合：在无监督学习时，提高模型的鲁棒性很重要，因此集成学习就大有用武之地。比如上面提到的Isolation Forest，就是基于构建多棵决策树实现的。最早的集成检测框架feature bagging[9]与分类问题中的随机森林（random forest）很像，先将训练数据随机划分（每次选取所有样本的d/2-d个特征，d代表特征数），得到多个子训练集，再在每个训练集上训练一个独立的模型（默认为LOF）并最终合并所有的模型结果（如通过平均）。值得注意的是，因为没有标签，异常检测往往是通过bagging和feature bagging比较多，而boosting比较少见。boosting情况下的异常检测，一般需要生成伪标签，可参靠[13, 14]。集成异常检测是一个新兴但很有趣的领域，综述文章可以参考[16, 17, 18]。

| •	特定领域上的异常检测：比如图像异常检测 [21]，顺序及流数据异常检测（时间序列异常检测）[22]，以及高维空间上的异常检测 [23]，比如前文提到的Isolation Forest就很适合高维数据上的异常检测。


| 维度低的时候，二维 可以直接用高斯函数的3希格玛原则，低维，KNN，实际上是计算相似度，再高维的话可以isolation Forrest， 之后两个月我可以学一下  （pca或者autoencoder降维 再高斯）

sklearn

https://scikit-learn.org/stable/modules/outlier_detection.html#overview-of-outlier-detection-methods


上采用 & 下采样
---------------------
https://www.cnblogs.com/zhanjiahui/p/11643544.html

https://www.jianshu.com/p/fd9e2166cfcc

各种距离 欧式 余弦
-----------------------

启发式 生成式算法
-----------------------------

L0 L1 L2 正则化
-------------------
| L0正则化的值是模型参数中非零参数的个数。
| L1正则化表示各个参数绝对值之和。
| L2正则化标识各个参数的平方的和的开方值。

| L1 和 L2：
| •	L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度。
| •	L2 计算起来更方便，而 L1 在特别是非稀疏向量上的计算效率就很低；
| •	L1 最重要的一个特点，输出稀疏，会把不重要的特征直接置零，而 L2 则不会；
| •	L2 有唯一解，而 L1 不是。


Rank Averaging
-----------------------------

.. image:: ../../_static/machine_learning/Rank_Averaging.png
	:align: center

数据清洗
-----------------------
| 数据清洗(Data cleaning)– 对数据进行重新审查和校验的过程，目的在于删除重复信息、纠正存在的错误，并提供数据一致性。
| 按照一定的规则把“脏”的“洗掉”，指发现并纠正数据文件中可识别的错误的最后一道程序，包括检查数据一致性，处理无效值和缺失值等。
| 不符合要求的数据主要是有不完整的数据、错误的数据、重复的数据三大类。
| 数据清洗是与问卷审核不同，录入后的数据清理一般是由计算机而不是人工完成


推荐系统
=====================
light GBM, Deepfm,Wide&Deep,YoutubeNet, PNN

